{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RESTART & RUN ALL IN THIS NOTEBOOK. it will mess stuff up\n",
    "#LDA\n",
    "#Latent Dirichlet Allocation\n",
    "#latent means hidden. Dirich is a type of probaility distribution\n",
    "#we will attempt to find hiden topics from probability distributions\n",
    "#Every document consists of a mix of topics. Every topic consist of a mix of words\n",
    "#we want LDA to learn the mix of topics in each document, and the mix of words in each topic.\n",
    "#We choose a number of topics we think our documents have.\n",
    "#it randomly assigns each word in each document to a random topic\n",
    "#it goes through each word and its assigned topic in each document. it looks at how often the topic\n",
    "#occurs in the document and how often the word ocurs in the topic. based on this it might assign a new topic for the word.repeat.\n",
    "#this is done through an import called gensim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>abc</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>aboveaverage</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>yukon</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymail</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsau</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theguardian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestar</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 4836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aap  abc  abide  ability  able  aboard  aboveaverage  abroad  \\\n",
       "bbc            0    0      0        2     2       0             0       0   \n",
       "dailymail      0    0      0        1    18       0             0       1   \n",
       "fox            0    0      1        0     1       1             0       0   \n",
       "newsau         4    1      0        0     0       0             1       0   \n",
       "theguardian    0    0      0        1     1       0             0       1   \n",
       "thestar        0    0      0        0     6       0             0       0   \n",
       "\n",
       "             absence  absolute  ...  young  younger  youre  youth  youve  \\\n",
       "bbc                0         0  ...      4        0      0      0      0   \n",
       "dailymail          1         3  ...      0        0      2      0      2   \n",
       "fox                0         0  ...      1        0      5      1      0   \n",
       "newsau             2         0  ...      0        0      0      0      0   \n",
       "theguardian        0         0  ...      6        1      3      0      0   \n",
       "thestar            0         0  ...      0        0      4      0      0   \n",
       "\n",
       "             yukon  zealand  zero  zoological  zoom  \n",
       "bbc              0        0     0           1     0  \n",
       "dailymail        0        0     1           0     0  \n",
       "fox              0        1     0           0     0  \n",
       "newsau           0        0     0           0     0  \n",
       "theguardian      0        0     1           0     0  \n",
       "thestar          1        0     1           0     1  \n",
       "\n",
       "[6 rows x 4836 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "data = pd.read_pickle('dtmsix.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbc</th>\n",
       "      <th>dailymail</th>\n",
       "      <th>fox</th>\n",
       "      <th>newsau</th>\n",
       "      <th>theguardian</th>\n",
       "      <th>thestar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aap</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bbc  dailymail  fox  newsau  theguardian  thestar\n",
       "aap        0          0    0       4            0        0\n",
       "abc        0          0    0       1            0        0\n",
       "abide      0          0    1       0            0        0\n",
       "ability    2          1    0       0            1        0\n",
       "able       2         18    1       0            1        6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import matutils, models #conda install -c conda-forge gensim  #lots of weird packages in this one...\n",
    "import scipy.sparse\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level-logging.INFO)\n",
    "\n",
    "#lda needs a term-document matrix, so we transpose our document-term matrix to get that.\n",
    "tdm = data.transpose()\n",
    "tdm.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lda we need a sparse matrix and then a specific gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv= pickle.load(open(\"cvsix.pkl\", \"rb\")) #gensim requires a dict of every single term and their location in the tdm. \n",
    "id2word = dict((v,k) for k,v in cv.vocabulary_.items()) #our cv has that, and the id for every single column in this vocabulary items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"said\" + 0.008*\"people\" + 0.007*\"coronavirus\" + 0.007*\"virus\" + 0.006*\"new\" + 0.003*\"china\" + 0.003*\"pandemic\" + 0.003*\"infected\" + 0.003*\"infection\" + 0.003*\"social\"'),\n",
       " (1,\n",
       "  '0.013*\"app\" + 0.011*\"said\" + 0.007*\"people\" + 0.006*\"government\" + 0.006*\"health\" + 0.005*\"nhs\" + 0.005*\"coronavirus\" + 0.005*\"contact\" + 0.005*\"virus\" + 0.005*\"data\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have everything needed now. we need to also specify number of topics and number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"virus\" + 0.011*\"app\" + 0.010*\"said\" + 0.009*\"people\" + 0.008*\"coronavirus\" + 0.005*\"government\" + 0.005*\"health\" + 0.004*\"china\" + 0.004*\"vaccine\" + 0.003*\"risk\"'),\n",
       " (1,\n",
       "  '0.017*\"said\" + 0.010*\"care\" + 0.007*\"homes\" + 0.007*\"health\" + 0.006*\"longterm\" + 0.006*\"residents\" + 0.005*\"march\" + 0.005*\"leave\" + 0.005*\"home\" + 0.005*\"staff\"'),\n",
       " (2,\n",
       "  '0.017*\"app\" + 0.010*\"nhs\" + 0.009*\"contact\" + 0.009*\"people\" + 0.008*\"data\" + 0.007*\"said\" + 0.006*\"coronavirus\" + 0.006*\"government\" + 0.005*\"symptoms\" + 0.005*\"health\"'),\n",
       " (3,\n",
       "  '0.009*\"new\" + 0.008*\"coronavirus\" + 0.008*\"said\" + 0.007*\"people\" + 0.005*\"homeless\" + 0.005*\"pandemic\" + 0.005*\"social\" + 0.004*\"china\" + 0.004*\"york\" + 0.004*\"city\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#more topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"app\" + 0.008*\"people\" + 0.008*\"said\" + 0.008*\"nhs\" + 0.007*\"contact\" + 0.007*\"government\" + 0.007*\"data\" + 0.006*\"coronavirus\" + 0.005*\"health\" + 0.005*\"virus\"'),\n",
       " (1,\n",
       "  '0.016*\"said\" + 0.006*\"people\" + 0.006*\"virus\" + 0.006*\"care\" + 0.006*\"new\" + 0.005*\"coronavirus\" + 0.005*\"health\" + 0.005*\"home\" + 0.004*\"homes\" + 0.004*\"pandemic\"'),\n",
       " (2,\n",
       "  '0.010*\"app\" + 0.008*\"people\" + 0.007*\"coronavirus\" + 0.006*\"virus\" + 0.004*\"said\" + 0.003*\"trial\" + 0.003*\"drugs\" + 0.003*\"evidence\" + 0.003*\"tests\" + 0.003*\"users\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3topics, 40 passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=40)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with only nouns\n",
    "from nltk import word_tokenize, pos_tag \n",
    "import nltk\n",
    "#nltk.download('punkt') #run this\n",
    "#nltk.download('averaged_perceptron_tagger') #run this\n",
    "\n",
    "def nouns(text):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN' #NN is nouns, JJ is adjectives see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word,pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textbits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>the uks test track and trace coronavirus strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymail</th>\n",
       "      <td>by kellie chudzinski for dailymailcom   publis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>michigan governor doubles down on state of eme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsau</th>\n",
       "      <td>a vaccine is widely acknowledged as the key to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theguardian</th>\n",
       "      <td>more people are staying indoors to avoid conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestar</th>\n",
       "      <td>vicki mckenna came to the ministry of healths ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      textbits\n",
       "bbc          the uks test track and trace coronavirus strat...\n",
       "dailymail    by kellie chudzinski for dailymailcom   publis...\n",
       "fox          michigan governor doubles down on state of eme...\n",
       "newsau       a vaccine is widely acknowledged as the key to...\n",
       "theguardian  more people are staying indoors to avoid conta...\n",
       "thestar      vicki mckenna came to the ministry of healths ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_pickle('data_clean_six.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textbits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>test track trace coronavirus strategy health s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymail</th>\n",
       "      <td>kellie chudzinski dailymailcom comments monday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>michigan governor state emergency tobin report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsau</th>\n",
       "      <td>vaccine key pandemic benefits country coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theguardian</th>\n",
       "      <td>people indoors contact people light report cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestar</th>\n",
       "      <td>vicki mckenna ministry healths offices preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      textbits\n",
       "bbc          test track trace coronavirus strategy health s...\n",
       "dailymail    kellie chudzinski dailymailcom comments monday...\n",
       "fox          michigan governor state emergency tobin report...\n",
       "newsau       vaccine key pandemic benefits country coronavi...\n",
       "theguardian  people indoors contact people light report cen...\n",
       "thestar      vicki mckenna ministry healths offices preside..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use nouns function\n",
    "data_nouns = pd.DataFrame(data_clean.textbits.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>aboveaverage</th>\n",
       "      <th>absence</th>\n",
       "      <th>abundance</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>accommodations</th>\n",
       "      <th>account</th>\n",
       "      <th>accusations</th>\n",
       "      <th>aches</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>yorkers</th>\n",
       "      <th>yorks</th>\n",
       "      <th>yorkshire</th>\n",
       "      <th>youll</th>\n",
       "      <th>yukon</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymail</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsau</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theguardian</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestar</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 2592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ability  aboveaverage  absence  abundance  access  accident  \\\n",
       "bbc                2             0        0          0       0         0   \n",
       "dailymail          1             0        0          1       7         0   \n",
       "fox                0             0        0          0       1         0   \n",
       "newsau             0             1        2          0       3         2   \n",
       "theguardian        1             0        0          0       0         0   \n",
       "thestar            0             0        0          0       2         0   \n",
       "\n",
       "             accommodations  account  accusations  aches  ...  yesterday  \\\n",
       "bbc                       0        0            0      0  ...          0   \n",
       "dailymail                 0        2            0      0  ...          4   \n",
       "fox                       1        0            0      0  ...          0   \n",
       "newsau                    0        0            1      0  ...          0   \n",
       "theguardian               0        0            0      3  ...          0   \n",
       "thestar                   0        0            0      0  ...          0   \n",
       "\n",
       "             york  yorkers  yorks  yorkshire  youll  yukon  zealand  zero  \\\n",
       "bbc             0        0      0          0      0      0        0     0   \n",
       "dailymail       2        0      0          2      0      0        0     1   \n",
       "fox            12        3      1          0      0      0        1     0   \n",
       "newsau          2        0      0          0      0      0        0     0   \n",
       "theguardian     0        0      0          0      0      0        0     0   \n",
       "thestar         3        0      0          0      1      1        0     1   \n",
       "\n",
       "             zoom  \n",
       "bbc             0  \n",
       "dailymail       0  \n",
       "fox             0  \n",
       "newsau          0  \n",
       "theguardian     0  \n",
       "thestar         1  \n",
       "\n",
       "[6 rows x 2592 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new document term matrix with only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "add_stop_words=['like','im','know','just','dont','thats','right','people','youre','got','gonna','time','think','said',\n",
    "               'aap', 'week','ace2','yesyoustillhavetostayhome']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.textbits)\n",
    "data_dtmn=pd.DataFrame(data_cvn.toarray(),columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "#vocabulary dict\n",
    "id2wordn = dict((v,k)for k,v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"coronavirus\" + 0.006*\"york\" + 0.006*\"city\" + 0.005*\"news\" + 0.005*\"police\" + 0.005*\"china\" + 0.005*\"homeless\" + 0.004*\"distancing\" + 0.004*\"officers\" + 0.004*\"media\"'),\n",
       " (1,\n",
       "  '0.020*\"app\" + 0.012*\"virus\" + 0.010*\"health\" + 0.010*\"government\" + 0.010*\"coronavirus\" + 0.008*\"contact\" + 0.008*\"data\" + 0.007*\"risk\" + 0.006*\"symptoms\" + 0.006*\"care\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"care\" + 0.013*\"homes\" + 0.012*\"health\" + 0.010*\"residents\" + 0.009*\"home\" + 0.009*\"staff\" + 0.009*\"march\" + 0.008*\"cases\" + 0.008*\"maternity\" + 0.007*\"government\"'),\n",
       " (1,\n",
       "  '0.013*\"coronavirus\" + 0.007*\"york\" + 0.007*\"city\" + 0.006*\"news\" + 0.006*\"police\" + 0.006*\"china\" + 0.006*\"homeless\" + 0.005*\"distancing\" + 0.005*\"media\" + 0.005*\"officers\"'),\n",
       " (2,\n",
       "  '0.025*\"app\" + 0.014*\"virus\" + 0.012*\"coronavirus\" + 0.011*\"government\" + 0.011*\"contact\" + 0.010*\"data\" + 0.009*\"health\" + 0.008*\"risk\" + 0.007*\"symptoms\" + 0.006*\"nhs\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"virus\" + 0.016*\"app\" + 0.012*\"government\" + 0.010*\"coronavirus\" + 0.010*\"vaccine\" + 0.009*\"health\" + 0.008*\"china\" + 0.007*\"risk\" + 0.006*\"cases\" + 0.006*\"infection\"'),\n",
       " (1,\n",
       "  '0.024*\"app\" + 0.012*\"coronavirus\" + 0.012*\"contact\" + 0.012*\"data\" + 0.010*\"government\" + 0.008*\"health\" + 0.007*\"person\" + 0.007*\"nhs\" + 0.007*\"risk\" + 0.007*\"symptoms\"'),\n",
       " (2,\n",
       "  '0.017*\"app\" + 0.014*\"virus\" + 0.013*\"coronavirus\" + 0.007*\"trial\" + 0.007*\"drugs\" + 0.006*\"health\" + 0.006*\"data\" + 0.006*\"evidence\" + 0.006*\"users\" + 0.005*\"tests\"'),\n",
       " (3,\n",
       "  '0.019*\"care\" + 0.014*\"homes\" + 0.013*\"health\" + 0.011*\"residents\" + 0.010*\"staff\" + 0.010*\"home\" + 0.009*\"march\" + 0.008*\"cases\" + 0.008*\"maternity\" + 0.007*\"government\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next lda we can try nouns and adjectives\n",
    "#more passes will finetune it to reach a more steady point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_and_adj(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ' #NN is nouns, JJ is adjectives see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_and_adj = [word for (word,pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
    "    return ' '.join(nouns_and_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textbits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>uks test track trace coronavirus strategy nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymail</th>\n",
       "      <td>kellie chudzinski dailymailcom comments first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>michigan governor state emergency tobin report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsau</th>\n",
       "      <td>vaccine key pandemic enormous benefits country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theguardian</th>\n",
       "      <td>people indoors contact people light recent rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestar</th>\n",
       "      <td>vicki mckenna ministry healths offices january...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      textbits\n",
       "bbc          uks test track trace coronavirus strategy nati...\n",
       "dailymail    kellie chudzinski dailymailcom comments first ...\n",
       "fox          michigan governor state emergency tobin report...\n",
       "newsau       vaccine key pandemic enormous benefits country...\n",
       "theguardian  people indoors contact people light recent rep...\n",
       "thestar      vicki mckenna ministry healths offices january..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(data_clean.textbits.apply(nouns_and_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>aboveaverage</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abundance</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>accommodations</th>\n",
       "      <th>account</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>yukon</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailymail</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsau</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theguardian</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestar</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 3249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             abc  ability  aboveaverage  absence  absolute  abundance  access  \\\n",
       "bbc            0        2             0        0         0          0       0   \n",
       "dailymail      0        1             0        0         3          1       7   \n",
       "fox            0        0             0        0         0          0       1   \n",
       "newsau         1        0             1        2         0          0       3   \n",
       "theguardian    0        1             0        0         0          0       0   \n",
       "thestar        0        0             0        0         0          0       2   \n",
       "\n",
       "             accident  accommodations  account  ...  youll  young  younger  \\\n",
       "bbc                 0               0        0  ...      0      4        0   \n",
       "dailymail           0               0        2  ...      0      0        0   \n",
       "fox                 0               1        0  ...      0      1        0   \n",
       "newsau              2               0        0  ...      0      0        0   \n",
       "theguardian         0               0        0  ...      0      6        1   \n",
       "thestar             0               0        0  ...      1      0        0   \n",
       "\n",
       "             youth  youve  yukon  zealand  zero  zoological  zoom  \n",
       "bbc              0      0      0        0     0           1     0  \n",
       "dailymail        0      1      0        0     1           0     0  \n",
       "fox              1      0      0        1     0           0     0  \n",
       "newsau           0      0      0        0     0           0     0  \n",
       "theguardian      0      0      0        0     0           0     0  \n",
       "thestar          0      0      1        0     1           0     1  \n",
       "\n",
       "[6 rows x 3249 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new dtm with nouns and adj\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.textbits)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(),columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new gensim corpus for nouns and adj\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "#new vocabulary dictionary\n",
    "id2wordna = dict((v,k) for k,v in cvna.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"app\" + 0.011*\"nhs\" + 0.010*\"contact\" + 0.010*\"data\" + 0.005*\"symptoms\" + 0.005*\"mr\" + 0.005*\"users\" + 0.005*\"test\" + 0.004*\"phone\" + 0.004*\"isle\"'),\n",
       " (1,\n",
       "  '0.009*\"care\" + 0.008*\"app\" + 0.006*\"homes\" + 0.005*\"staff\" + 0.004*\"march\" + 0.004*\"longterm\" + 0.004*\"symptoms\" + 0.004*\"patients\" + 0.004*\"maternity\" + 0.004*\"ontario\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 topics first\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"app\" + 0.001*\"data\" + 0.001*\"nhs\" + 0.001*\"contact\" + 0.001*\"mr\" + 0.001*\"symptoms\" + 0.001*\"test\" + 0.001*\"phone\" + 0.000*\"hancock\" + 0.000*\"human\"'),\n",
       " (1,\n",
       "  '0.009*\"care\" + 0.006*\"homes\" + 0.005*\"march\" + 0.005*\"staff\" + 0.005*\"patients\" + 0.004*\"longterm\" + 0.004*\"symptoms\" + 0.004*\"app\" + 0.004*\"maternity\" + 0.004*\"trial\"'),\n",
       " (2,\n",
       "  '0.029*\"app\" + 0.013*\"nhs\" + 0.012*\"contact\" + 0.012*\"data\" + 0.007*\"mr\" + 0.006*\"symptoms\" + 0.005*\"users\" + 0.005*\"test\" + 0.005*\"phone\" + 0.004*\"human\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 topics first\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"app\" + 0.015*\"nhs\" + 0.013*\"contact\" + 0.013*\"data\" + 0.007*\"symptoms\" + 0.006*\"mr\" + 0.006*\"users\" + 0.006*\"test\" + 0.005*\"phone\" + 0.004*\"isle\"'),\n",
       " (1,\n",
       "  '0.021*\"app\" + 0.006*\"cent\" + 0.006*\"states\" + 0.005*\"picture\" + 0.005*\"united\" + 0.005*\"mr\" + 0.005*\"restrictions\" + 0.004*\"source\" + 0.004*\"global\" + 0.004*\"chinese\"'),\n",
       " (2,\n",
       "  '0.009*\"homeless\" + 0.007*\"york\" + 0.007*\"police\" + 0.006*\"monday\" + 0.005*\"distancing\" + 0.005*\"officers\" + 0.005*\"media\" + 0.004*\"chinese\" + 0.004*\"shelters\" + 0.004*\"ohio\"'),\n",
       " (3,\n",
       "  '0.014*\"care\" + 0.010*\"homes\" + 0.007*\"staff\" + 0.007*\"march\" + 0.007*\"longterm\" + 0.006*\"symptoms\" + 0.006*\"patients\" + 0.006*\"maternity\" + 0.006*\"ontario\" + 0.005*\"canada\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 topics first\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#er kommet til at køre nedenstående celle efter jeg havde lavet små ændringer i dataen og havde beskrevet den. Kæmpe fejl. \n",
    "#Laver nye celler fremover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"care\" + 0.013*\"homes\" + 0.009*\"march\" + 0.009*\"staff\" + 0.009*\"longterm\" + 0.008*\"maternity\" + 0.007*\"ontario\" + 0.006*\"nursing\" + 0.006*\"canada\" + 0.006*\"parental\"'),\n",
       " (1,\n",
       "  '0.029*\"app\" + 0.013*\"nhs\" + 0.012*\"contact\" + 0.012*\"data\" + 0.007*\"mr\" + 0.006*\"symptoms\" + 0.005*\"test\" + 0.005*\"phone\" + 0.005*\"users\" + 0.004*\"human\"'),\n",
       " (2,\n",
       "  '0.007*\"symptoms\" + 0.006*\"pankhania\" + 0.005*\"countries\" + 0.005*\"patients\" + 0.005*\"immune\" + 0.005*\"uk\" + 0.005*\"surface\" + 0.005*\"contact\" + 0.004*\"humans\" + 0.004*\"young\"'),\n",
       " (3,\n",
       "  '0.012*\"app\" + 0.005*\"homeless\" + 0.004*\"york\" + 0.004*\"trial\" + 0.004*\"police\" + 0.004*\"distancing\" + 0.004*\"drugs\" + 0.004*\"users\" + 0.004*\"data\" + 0.003*\"prof\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=120)\n",
    "ldana.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic 0: \n",
    "#Topic 1: \n",
    "#Topic 2: \n",
    "#Topic 3: \n",
    "\n",
    "#these  below would fit to an old version of our model.Read above, i accidently ran it again.\n",
    "#Topic 0: travels,countries\n",
    "#Topic 1: police, homeless people,metropolis\n",
    "#Topic 2: sickness, \n",
    "#Topic 3: Court, criminals, drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'bbc'),\n",
       " (1, 'dailymail'),\n",
       " (3, 'fox'),\n",
       " (1, 'newsau'),\n",
       " (2, 'theguardian'),\n",
       " (0, 'thestar')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these  below would fit to an old version of our model.Read above, i accidently ran the cell again with new data.\n",
    "#Topic 0: travels,countries (newsau)\n",
    "#Topic 1: police, homeless people,metropolis (fox)\n",
    "#Topic 2: sickness (dailymail, theguardian, thestar)\n",
    "#Topic 3: Court, criminals, drugs(bbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"west\" + 0.000*\"entire\" + 0.000*\"pressure\" + 0.000*\"greater\" + 0.000*\"hand\" + 0.000*\"reasons\" + 0.000*\"mobile\" + 0.000*\"exposure\" + 0.000*\"jobs\" + 0.000*\"addition\"'),\n",
       " (1,\n",
       "  '0.027*\"app\" + 0.016*\"nhs\" + 0.014*\"contact\" + 0.013*\"data\" + 0.008*\"symptoms\" + 0.007*\"mr\" + 0.006*\"test\" + 0.006*\"phone\" + 0.005*\"users\" + 0.004*\"isle\"'),\n",
       " (2,\n",
       "  '0.010*\"care\" + 0.009*\"app\" + 0.008*\"homes\" + 0.005*\"staff\" + 0.005*\"march\" + 0.005*\"longterm\" + 0.004*\"maternity\" + 0.004*\"ontario\" + 0.004*\"cent\" + 0.004*\"states\"'),\n",
       " (3,\n",
       "  '0.012*\"app\" + 0.007*\"drugs\" + 0.006*\"contact\" + 0.005*\"symptoms\" + 0.005*\"uk\" + 0.005*\"countries\" + 0.005*\"trial\" + 0.005*\"patients\" + 0.005*\"nhs\" + 0.004*\"humans\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=500)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"care\" + 0.008*\"homes\" + 0.006*\"staff\" + 0.005*\"march\" + 0.005*\"longterm\" + 0.005*\"patients\" + 0.005*\"maternity\" + 0.005*\"symptoms\" + 0.004*\"ontario\" + 0.004*\"canada\"'),\n",
       " (1,\n",
       "  '0.019*\"app\" + 0.006*\"cent\" + 0.005*\"states\" + 0.005*\"united\" + 0.005*\"picture\" + 0.004*\"restrictions\" + 0.004*\"mr\" + 0.004*\"global\" + 0.004*\"chinese\" + 0.004*\"source\"'),\n",
       " (2,\n",
       "  '0.027*\"app\" + 0.014*\"nhs\" + 0.012*\"contact\" + 0.012*\"data\" + 0.007*\"symptoms\" + 0.006*\"mr\" + 0.006*\"test\" + 0.006*\"users\" + 0.005*\"phone\" + 0.004*\"isle\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=1000)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic 0: hospital, healthcare\n",
    "#Topic 1: travels, countries\n",
    "#Topic 2: phones,apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'bbc'),\n",
       " (2, 'dailymail'),\n",
       " (0, 'fox'),\n",
       " (1, 'newsau'),\n",
       " (0, 'theguardian'),\n",
       " (0, 'thestar')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic 0: hospital, healthcare(fox,theguardian,thestar)\n",
    "#Topic 1: travels, countries(newsau)\n",
    "#Topic 2: phones,apps(bbc,dailymail)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
